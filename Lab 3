import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

# ðŸ”¹ Load CIFAR-10 dataset
dataset, info = tfds.load('cifar10', with_info=True, as_supervised=True)
train_data, test_data = dataset['train'], dataset['test']

# ðŸ”¹ Preprocess function
def preprocess_image(image, label):
    image = tf.image.resize(image, (32, 32))  # Resize (if needed)
    image = tf.cast(image, tf.float32) / 255.0  # Normalize [0,1]
    return image, label

# ðŸ”¹ Apply preprocessing
train_data = train_data.map(preprocess_image).batch(128).shuffle(10000)
test_data = test_data.map(preprocess_image).batch(128)

# ðŸ”¹ Display a sample image
plt.imshow(next(iter(train_data))[0][0])
plt.axis('off')
plt.show()

# ðŸ”¹ VAE Model Parameters
original_dim = (32, 32, 3)  # CIFAR-10 image size
latent_dim = 10  # Latent space size

# ðŸ”¹ Build Encoder
inputs = layers.Input(shape=original_dim)
x = layers.Conv2D(32, (3, 3), activation='relu', strides=2, padding='same')(inputs)
x = layers.Conv2D(64, (3, 3), activation='relu', strides=2, padding='same')(x)
x = layers.Conv2D(128, (3, 3), activation='relu', strides=2, padding='same')(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)

# ðŸ”¹ Latent space
z_mean = layers.Dense(latent_dim)(x)
z_log_var = layers.Dense(latent_dim)(x)

# ðŸ”¹ Sampling layer
def sampling(args):
    z_mean, z_log_var = args
    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])

# ðŸ”¹ Decoder
latent_inputs = layers.Input(shape=(latent_dim,))
x = layers.Dense(4 * 4 * 128, activation='relu')(latent_inputs)
x = layers.Reshape((4, 4, 128))(x)
x = layers.Conv2DTranspose(128, (3, 3), activation='relu', strides=2, padding='same')(x)
x = layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same')(x)
x = layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same')(x)
decoded = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)

# ðŸ”¹ Define VAE model
vae = models.Model(inputs, decoded)

# ðŸ”¹ VAE Loss
xent_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(inputs, decoded), axis=(1, 2)))
kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))
vae_loss = xent_loss + kl_loss

vae.add_loss(vae_loss)
vae.compile(optimizer='adam')

# ðŸ”¹ Train VAE
vae.fit(train_data, epochs=10, validation_data=test_data)

# ðŸ”¹ Create Generator Model
decoder = models.Model(latent_inputs, decoded)

# ðŸ”¹ Generate New Images
def generate_images(n=10):
    random_latent_vectors = np.random.normal(size=(n, latent_dim))
    generated_images = decoder.predict(random_latent_vectors)

    plt.figure(figsize=(15, 5))
    for i in range(n):
        ax = plt.subplot(2, 5, i + 1)
        plt.imshow(generated_images[i])
        plt.axis('off')
    plt.show()

generate_images(n=10)  # Generate 10 new images

# ðŸ”¹ Encoder Model
encoder = models.Model(inputs, z_mean)

# ðŸ”¹ Visualize 2D Latent Space
latent_representations = encoder.predict(test_data.map(lambda x, y: x))

plt.figure(figsize=(8, 6))
plt.scatter(latent_representations[:, 0], latent_representations[:, 1], c='blue', alpha=0.5)
plt.title('Latent Space Representation')
plt.xlabel('z[0]')
plt.ylabel('z[1]')
plt.show()
