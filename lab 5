from torchvision.datasets import CIFAR10
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)
class RBM(nn.Module):
    def __init__(self, visible_units, hidden_units):
        super(RBM, self).__init__()
        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.01)
        self.h_bias = nn.Parameter(torch.zeros(hidden_units))
        self.v_bias = nn.Parameter(torch.zeros(visible_units))

    def sample_prob(self, probs):
        return torch.bernoulli(probs)

    def v_to_h(self, v):
        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))
        return p_h, self.sample_prob(p_h)

    def h_to_v(self, h):
        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))
        return p_v, self.sample_prob(p_v)

    def contrastive_divergence(self, v0, lr=0.01, k=1):
        v = v0
        for _ in range(k):
            h_prob, h = self.v_to_h(v)
            v_prob, v = self.h_to_v(h)

        h0_prob, _ = self.v_to_h(v0)
        hk_prob, _ = self.v_to_h(v)

        self.W.data += lr * (torch.matmul(h0_prob.t(), v0) - torch.matmul(hk_prob.t(), v)) / v0.size(0)
        self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)
        self.h_bias.data += lr * torch.sum(h0_prob - hk_prob, dim=0) / v0.size(0)
rbm1 = RBM(3072, 1024)
rbm2 = RBM(1024, 512)

print("Training RBM 1...")
for epoch in range(5):
    for batch, _ in train_loader:
        x = batch.view(-1, 3072)
        rbm1.contrastive_divergence(x)

print("Training RBM 2...")
for epoch in range(5):
    for batch, _ in train_loader:
        x = batch.view(-1, 3072)
        h1, _ = rbm1.v_to_h(x)
        rbm2.contrastive_divergence(h1)
class DBN(nn.Module):
    def __init__(self):
        super(DBN, self).__init__()
        self.fc1 = nn.Linear(3072, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.classifier = nn.Linear(512, 10)
        self.relu = nn.ReLU()

        self.fc1.weight.data = rbm1.W.data
        self.fc1.bias.data = rbm1.h_bias.data
        self.fc2.weight.data = rbm2.W.data
        self.fc2.bias.data = rbm2.h_bias.data

    def forward(self, x):
        x = x.view(-1, 3072)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.classifier(x)
dbn = DBN()
optimizer = optim.Adam(dbn.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
print("Fine-tuning DBN...")
for epoch in range(5):
    for batch, target in train_loader:
        optimizer.zero_grad()
        output = dbn(batch)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
class TraditionalNN(nn.Module):
    def __init__(self):
        super(TraditionalNN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(3072, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        return self.net(x.view(-1, 3072))

mlp = TraditionalNN()
optimizer_mlp = optim.Adam(mlp.parameters(), lr=0.001)
print("Training traditional NN...")
for epoch in range(5):
    for batch, target in train_loader:
        optimizer_mlp.zero_grad()
        output = mlp(batch)
        loss = criterion(output, target)
        loss.backward()
        optimizer_mlp.step()
def evaluate(model):
    correct = 0
    total = 0
    with torch.no_grad():
        for batch, target in test_loader:
            output = model(batch)
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    return correct / total

print(f"DBN Accuracy: {evaluate(dbn)*100:.2f}%")
print(f"Traditional NN Accuracy: {evaluate(mlp)*100:.2f}%")
